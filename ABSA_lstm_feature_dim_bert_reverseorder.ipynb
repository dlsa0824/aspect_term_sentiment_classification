{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先跑LSTM抓取feature(把hidden layer抽出來)，再放到bert裡面去做分類，先restaurant再laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 對處理好的laptop、restaurant的train、test資料作前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把dataframe裡的text切成text左邊跟右邊並做一些處理的function\n",
    "def split_text(df):\n",
    "    df['left_text'] = 'N/A'\n",
    "    df['right_text'] = 'N/A'\n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        text = df.loc[i, 'text']\n",
    "        aspect = df.loc[i, 'aspect']\n",
    "        text_split = text.split(aspect) # 根據aspect切割text左右邊\n",
    "        \n",
    "        left_text = text_split[0]+aspect\n",
    "        right_text = aspect+text_split[1]\n",
    "        left_text = left_text.lower() # 把字串變成小寫\n",
    "        right_text = right_text.lower()\n",
    "        left_text = re.sub('-', ' ', left_text)\n",
    "        right_text = re.sub('-', ' ', right_text)\n",
    "        left_text = re.sub('[.,!\"()#%&/:?~]', '', left_text) # 把字串中的一些符號刪除\n",
    "        right_text = re.sub('[.,!\"()#%&/:?~]', '', right_text)\n",
    "        \n",
    "        df.loc[i,'left_text'] = left_text\n",
    "        df.loc[i,'right_text'] = right_text\n",
    "        df.loc[i, 'left_right_text'] = left_text +' '+ right_text # 用來文字encoding\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7673/7673 [00:02<00:00, 2889.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練資料集: 5915\n",
      "測試資料集: 1758\n",
      "所有資料集: 7673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspect</th>\n",
       "      <th>polarity</th>\n",
       "      <th>left_text</th>\n",
       "      <th>right_text</th>\n",
       "      <th>left_right_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "      <td>but the staff</td>\n",
       "      <td>staff was so horrible to us</td>\n",
       "      <td>but the staff staff was so horrible to us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>to be completely fair the only redeeming facto...</td>\n",
       "      <td>food which was above average but couldn't make...</td>\n",
       "      <td>to be completely fair the only redeeming facto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>the food</td>\n",
       "      <td>food is uniformly exceptional with a very capa...</td>\n",
       "      <td>the food food is uniformly exceptional with a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>kitchen which will proudly whip up whatever yo...</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>menu or not</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Not only was the food outstanding, but the lit...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>not only was the food</td>\n",
       "      <td>food outstanding but the little 'perks' were g...</td>\n",
       "      <td>not only was the food food outstanding but the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Not only was the food outstanding, but the lit...</td>\n",
       "      <td>perks</td>\n",
       "      <td>positive</td>\n",
       "      <td>not only was the food outstanding but the litt...</td>\n",
       "      <td>perks' were great</td>\n",
       "      <td>not only was the food outstanding but the litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>orrechiete with sausage and chicken</td>\n",
       "      <td>positive</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>orrechiete with sausage and chicken usually th...</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>waiters</td>\n",
       "      <td>positive</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>waiters are kind enough to split the dish in h...</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>meats</td>\n",
       "      <td>neutral</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>meats</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0               But the staff was so horrible to us.   \n",
       "1  To be completely fair, the only redeeming fact...   \n",
       "2  The food is uniformly exceptional, with a very...   \n",
       "3  The food is uniformly exceptional, with a very...   \n",
       "4  The food is uniformly exceptional, with a very...   \n",
       "5  Not only was the food outstanding, but the lit...   \n",
       "6  Not only was the food outstanding, but the lit...   \n",
       "7  Our agreed favorite is the orrechiete with sau...   \n",
       "8  Our agreed favorite is the orrechiete with sau...   \n",
       "9  Our agreed favorite is the orrechiete with sau...   \n",
       "\n",
       "                                aspect  polarity  \\\n",
       "0                                staff  negative   \n",
       "1                                 food  positive   \n",
       "2                                 food  positive   \n",
       "3                              kitchen  positive   \n",
       "4                                 menu   neutral   \n",
       "5                                 food  positive   \n",
       "6                                perks  positive   \n",
       "7  orrechiete with sausage and chicken  positive   \n",
       "8                              waiters  positive   \n",
       "9                                meats   neutral   \n",
       "\n",
       "                                           left_text  \\\n",
       "0                                      but the staff   \n",
       "1  to be completely fair the only redeeming facto...   \n",
       "2                                           the food   \n",
       "3  the food is uniformly exceptional with a very ...   \n",
       "4  the food is uniformly exceptional with a very ...   \n",
       "5                              not only was the food   \n",
       "6  not only was the food outstanding but the litt...   \n",
       "7  our agreed favorite is the orrechiete with sau...   \n",
       "8  our agreed favorite is the orrechiete with sau...   \n",
       "9  our agreed favorite is the orrechiete with sau...   \n",
       "\n",
       "                                          right_text  \\\n",
       "0                        staff was so horrible to us   \n",
       "1  food which was above average but couldn't make...   \n",
       "2  food is uniformly exceptional with a very capa...   \n",
       "3  kitchen which will proudly whip up whatever yo...   \n",
       "4                                        menu or not   \n",
       "5  food outstanding but the little 'perks' were g...   \n",
       "6                                  perks' were great   \n",
       "7  orrechiete with sausage and chicken usually th...   \n",
       "8  waiters are kind enough to split the dish in h...   \n",
       "9                                              meats   \n",
       "\n",
       "                                     left_right_text  \n",
       "0          but the staff staff was so horrible to us  \n",
       "1  to be completely fair the only redeeming facto...  \n",
       "2  the food food is uniformly exceptional with a ...  \n",
       "3  the food is uniformly exceptional with a very ...  \n",
       "4  the food is uniformly exceptional with a very ...  \n",
       "5  not only was the food food outstanding but the...  \n",
       "6  not only was the food outstanding but the litt...  \n",
       "7  our agreed favorite is the orrechiete with sau...  \n",
       "8  our agreed favorite is the orrechiete with sau...  \n",
       "9  our agreed favorite is the orrechiete with sau...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_train = pd.read_csv('dataset/laptop_train_processed.csv', encoding='utf-8')\n",
    "restaurant_train = pd.read_csv('dataset/restaurant_train_processed.csv', encoding='utf-8')\n",
    "laptop_test = pd.read_csv('dataset/laptop_test_processed.csv', encoding='utf-8')\n",
    "restaurant_test = pd.read_csv('dataset/restaurant_test_processed.csv', encoding='utf-8')\n",
    "\n",
    "# 把train的資料串在一起\n",
    "train_data = restaurant_train.append(laptop_train)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "#把test的資料串在一起\n",
    "test_data = restaurant_test.append(laptop_test)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "#把train、test資料串在一起\n",
    "data = train_data.append(test_data)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# data切割text\n",
    "data = split_text(data)\n",
    "\n",
    "print('訓練資料集:', len(train_data))\n",
    "print('測試資料集:', len(test_data))\n",
    "print('所有資料集:', len(data))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\n",
      "\n",
      "the food is uniformly exceptional with a very capable kitchen\n",
      "\n",
      "kitchen which will proudly whip up whatever you feel like eating whether it's on the menu or not\n",
      "\n",
      "the food is uniformly exceptional with a very capable kitchen kitchen which will proudly whip up whatever you feel like eating whether it's on the menu or not\n"
     ]
    }
   ],
   "source": [
    "# print一個出來看看\n",
    "n = 3\n",
    "print(data.loc[n, 'text'])\n",
    "print()\n",
    "print(data.loc[n, 'left_text'])\n",
    "print()\n",
    "print(data.loc[n, 'right_text'])\n",
    "print()\n",
    "print(data.loc[n, 'left_right_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspect</th>\n",
       "      <th>polarity</th>\n",
       "      <th>left_text</th>\n",
       "      <th>right_text</th>\n",
       "      <th>left_right_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "      <td>but the staff</td>\n",
       "      <td>staff was so horrible to us</td>\n",
       "      <td>but the staff staff was so horrible to us</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>to be completely fair the only redeeming facto...</td>\n",
       "      <td>food which was above average but couldn't make...</td>\n",
       "      <td>to be completely fair the only redeeming facto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>the food</td>\n",
       "      <td>food is uniformly exceptional with a very capa...</td>\n",
       "      <td>the food food is uniformly exceptional with a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>kitchen which will proudly whip up whatever yo...</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>menu or not</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Not only was the food outstanding, but the lit...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>not only was the food</td>\n",
       "      <td>food outstanding but the little 'perks' were g...</td>\n",
       "      <td>not only was the food food outstanding but the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Not only was the food outstanding, but the lit...</td>\n",
       "      <td>perks</td>\n",
       "      <td>positive</td>\n",
       "      <td>not only was the food outstanding but the litt...</td>\n",
       "      <td>perks' were great</td>\n",
       "      <td>not only was the food outstanding but the litt...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>orrechiete with sausage and chicken</td>\n",
       "      <td>positive</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>orrechiete with sausage and chicken usually th...</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>waiters</td>\n",
       "      <td>positive</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>waiters are kind enough to split the dish in h...</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>meats</td>\n",
       "      <td>neutral</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>meats</td>\n",
       "      <td>our agreed favorite is the orrechiete with sau...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0               But the staff was so horrible to us.   \n",
       "1  To be completely fair, the only redeeming fact...   \n",
       "2  The food is uniformly exceptional, with a very...   \n",
       "3  The food is uniformly exceptional, with a very...   \n",
       "4  The food is uniformly exceptional, with a very...   \n",
       "5  Not only was the food outstanding, but the lit...   \n",
       "6  Not only was the food outstanding, but the lit...   \n",
       "7  Our agreed favorite is the orrechiete with sau...   \n",
       "8  Our agreed favorite is the orrechiete with sau...   \n",
       "9  Our agreed favorite is the orrechiete with sau...   \n",
       "\n",
       "                                aspect  polarity  \\\n",
       "0                                staff  negative   \n",
       "1                                 food  positive   \n",
       "2                                 food  positive   \n",
       "3                              kitchen  positive   \n",
       "4                                 menu   neutral   \n",
       "5                                 food  positive   \n",
       "6                                perks  positive   \n",
       "7  orrechiete with sausage and chicken  positive   \n",
       "8                              waiters  positive   \n",
       "9                                meats   neutral   \n",
       "\n",
       "                                           left_text  \\\n",
       "0                                      but the staff   \n",
       "1  to be completely fair the only redeeming facto...   \n",
       "2                                           the food   \n",
       "3  the food is uniformly exceptional with a very ...   \n",
       "4  the food is uniformly exceptional with a very ...   \n",
       "5                              not only was the food   \n",
       "6  not only was the food outstanding but the litt...   \n",
       "7  our agreed favorite is the orrechiete with sau...   \n",
       "8  our agreed favorite is the orrechiete with sau...   \n",
       "9  our agreed favorite is the orrechiete with sau...   \n",
       "\n",
       "                                          right_text  \\\n",
       "0                        staff was so horrible to us   \n",
       "1  food which was above average but couldn't make...   \n",
       "2  food is uniformly exceptional with a very capa...   \n",
       "3  kitchen which will proudly whip up whatever yo...   \n",
       "4                                        menu or not   \n",
       "5  food outstanding but the little 'perks' were g...   \n",
       "6                                  perks' were great   \n",
       "7  orrechiete with sausage and chicken usually th...   \n",
       "8  waiters are kind enough to split the dish in h...   \n",
       "9                                              meats   \n",
       "\n",
       "                                     left_right_text  label  \n",
       "0          but the staff staff was so horrible to us      0  \n",
       "1  to be completely fair the only redeeming facto...      2  \n",
       "2  the food food is uniformly exceptional with a ...      2  \n",
       "3  the food is uniformly exceptional with a very ...      2  \n",
       "4  the food is uniformly exceptional with a very ...      1  \n",
       "5  not only was the food food outstanding but the...      2  \n",
       "6  not only was the food outstanding but the litt...      2  \n",
       "7  our agreed favorite is the orrechiete with sau...      2  \n",
       "8  our agreed favorite is the orrechiete with sau...      2  \n",
       "9  our agreed favorite is the orrechiete with sau...      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把文字Label變成數字label\n",
    "data.loc[data['polarity'] == 'positive', 'label'] = 2\n",
    "data.loc[data['polarity'] == 'neutral', 'label'] = 1\n",
    "data.loc[data['polarity'] == 'negative', 'label'] = 0\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_text與right_text最多的字數: 72\n"
     ]
    }
   ],
   "source": [
    "#找出left_text跟right_text裡面最多是多少字\n",
    "max_count = 0\n",
    "for i in range(len(data)):\n",
    "    left_text_word_count = len(data.loc[i,'left_text'].split())\n",
    "    right_text_word_count = len(data.loc[i,'right_text'].split())\n",
    "    big_count = max(left_text_word_count, right_text_word_count)\n",
    "    if big_count>max_count:\n",
    "        max_count = big_count\n",
    "print('left_text與right_text最多的字數:', max_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 對文字做encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 7000 # 最大的字數\n",
    "max_seq_length = 80 # 句子最長長度\n",
    "embedding_dim = 300 # 每個字維度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6557 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# 把字變成token\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(data['left_right_text'].to_numpy())\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# word_index就是根據left_right_text內容彙整出來的切字跟代表那個字的token number (每個字的dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the : 1\n",
      "and : 2\n",
      "a : 3\n",
      "to : 4\n",
      "is : 5\n",
      "i : 6\n",
      "of : 7\n",
      "for : 8\n",
      "food : 9\n",
      "it : 10\n"
     ]
    }
   ],
   "source": [
    "# 檢查word_index(dictionary)裡面的東西，前面是字，後面是token\n",
    "for x in list(word_index)[0:10]:\n",
    "    print (x, ':', word_index[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our agreed favorite is the orrechiete with sausage and chicken usually the waiters are kind enough to split the dish in half so you get to sample both meats\n",
      "meats\n",
      "[79, 1973, 545, 5, 1, 2531, 12, 1779, 2, 89, 448, 1, 367, 19, 492, 253, 4, 1278, 1, 151, 13, 420, 32, 16, 58, 4, 1974, 222, 1048]\n",
      "[1048]\n",
      "<class 'list'>\n",
      "right text 倒過來\n",
      "[79, 1973, 545, 5, 1, 2531, 12, 1779, 2, 89, 448, 1, 367, 19, 492, 253, 4, 1278, 1, 151, 13, 420, 32, 16, 58, 4, 1974, 222, 1048]\n",
      "[1048]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 檢查其中一項字串的token\n",
    "n = 9 # index number\n",
    "left_text = data['left_text'].to_numpy() # 轉成向量\n",
    "right_text = data['right_text'].to_numpy()\n",
    "left_text_seq = tokenizer.texts_to_sequences(left_text)\n",
    "right_text_seq = tokenizer.texts_to_sequences(right_text)\n",
    "print(data.loc[n, 'left_text'])\n",
    "print(data.loc[n, 'right_text'])\n",
    "print(left_text_seq[n])\n",
    "print(right_text_seq[n])\n",
    "print(type(right_text_seq))\n",
    "# 把右邊的字串token倒過來，因為要從後面讀到前面\n",
    "print('right text 倒過來')\n",
    "for i in range(len(right_text_seq)):\n",
    "    right_text_seq[i] = right_text_seq[i][::-1]\n",
    "print(left_text_seq[n])\n",
    "print(right_text_seq[n])\n",
    "print(type(right_text_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  79 1973  545    5    1 2531   12 1779    2   89  448    1  367   19\n",
      "  492  253    4 1278    1  151   13  420   32   16   58    4 1974  222\n",
      " 1048    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "[1048    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# token sequence 後面補0的方法\n",
    "def text_seq_padding(text_seq):\n",
    "    if len(text_seq) < max_seq_length:\n",
    "        n = max_seq_length - len(text_seq)\n",
    "        text_seq = np.pad(text_seq, (0, n), mode ='constant', constant_values=(0)) # array右邊append n 個 0\n",
    "    return text_seq\n",
    "# 把每個left_text_seq，right_text_seq padding到同樣的長度 (後面補0)\n",
    "left_text_seq = [text_seq_padding(i) for i in left_text_seq] # 必須要 [ ] 輸出是list\n",
    "left_text_seq = np.array(left_text_seq)\n",
    "\n",
    "right_text_seq = [text_seq_padding(i) for i in right_text_seq]\n",
    "right_text_seq = np.array(right_text_seq)\n",
    "\n",
    "# n = 0 # index number\n",
    "print(left_text_seq[n])\n",
    "print(right_text_seq[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用預先處理的詞向量 (crawl 300 dim)\n",
    "#### https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(6558, 300)\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.0231      0.017       0.0157     ...  0.0744     -0.1118\n",
      "   0.0963    ]\n",
      " [-0.1081      0.0191      0.0354     ...  0.1104      0.0475\n",
      "  -0.0599    ]\n",
      " ...\n",
      " [ 0.16580001 -0.0169     -0.4138     ...  0.0933     -0.1168\n",
      "  -0.1777    ]\n",
      " [-0.1179      0.0726     -0.005      ...  0.2079      0.0322\n",
      "  -0.26879999]\n",
      " [ 0.24439999  0.1206      0.1123     ... -0.147      -0.0186\n",
      "  -0.3204    ]]\n"
     ]
    }
   ],
   "source": [
    "# 把embedding_matrix load 近來\n",
    "embedding_matrix = np.load('dataset/embedding_matrix.npy')\n",
    "print(type(embedding_matrix))\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認資料、並切割成train、test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bread\n",
      "bread is top notch as well\n",
      "[  1 305   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0]\n",
      "[  71   30 1074  358    5  305    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "\n",
      "boot time\n",
      "boot time is super fast around anywhere from 35 seconds to 1 minute\n",
      "[506  98   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0]\n",
      "[1318  445    4 1041 1983   44  846  260  139  525    5   98  506    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 稽查dataframe、token sequence裡面laptop_test、restaurant_test資料是否一致\n",
    "# laptop_test第一筆在5915；restaurant_test第一筆在6553\n",
    "print(data.loc[5915, 'left_text'])\n",
    "print(data.loc[5915, 'right_text'])\n",
    "print(left_text_seq[5915])\n",
    "print(right_text_seq[5915])\n",
    "print()\n",
    "print(data.loc[7035, 'left_text'])\n",
    "print(data.loc[7035, 'right_text'])\n",
    "print(left_text_seq[7035])\n",
    "print(right_text_seq[7035])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (7673,)\n",
      "0 0\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "1 1\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "# 把label轉成numpy\n",
    "Y = data['label'].to_numpy()\n",
    "print('Shape of Y:', Y.shape)\n",
    "for i in range(10):\n",
    "    print(data.loc[i, 'label'], Y[i])\n",
    "#[1 0 0] = negative\n",
    "#[0 1 0] = neutral\n",
    "#[0 0 1] = positve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5915 5915 5915\n",
      "1758 1758 1758\n"
     ]
    }
   ],
   "source": [
    "#把資料切割成train、test\n",
    "X_left_train = left_text_seq[:5915]\n",
    "X_right_train = right_text_seq[:5915]\n",
    "Y_train = Y[:5915]\n",
    "X_left_test = left_text_seq[5915:]\n",
    "X_right_test = right_text_seq[5915:]\n",
    "Y_test = Y[5915:]\n",
    "print(len(X_left_train), len(X_right_train), len(Y_train))\n",
    "print(len(X_left_test), len(X_right_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant_test     laptop_test\n",
      "positive 2 2        positive 2 2\n",
      "negative 2 2        positive 0 0\n",
      "positive 2 2        positive 2 2\n",
      "negative 2 2        positive 0 0\n",
      "negative 2 2        positive 0 0\n",
      "negative 2 2        positive 0 0\n",
      "positive 2 2        positive 2 2\n",
      "negative 2 2        positive 0 0\n",
      "neutral 2 2        positive 1 1\n",
      "positive 2 2        positive 2 2\n",
      "positive 1 1        neutral 2 2\n",
      "positive 2 2        positive 2 2\n",
      "positive 2 2        positive 2 2\n",
      "positive 2 2        positive 2 2\n",
      "positive 0 0        negative 2 2\n",
      "positive 2 2        positive 2 2\n",
      "negative 1 1        neutral 0 0\n",
      "negative 1 1        neutral 0 0\n",
      "positive 2 2        positive 2 2\n",
      "positive 2 2        positive 2 2\n"
     ]
    }
   ],
   "source": [
    "# 檢查polarity跟label有沒有不一樣\n",
    "print('restaurant_test', '   ','laptop_test')\n",
    "for i in range(20):\n",
    "    print(laptop_test.loc[i, 'polarity'], data.loc[5915+i, 'label'], Y_test[i], '      ', restaurant_test.loc[i, 'polarity'], data.loc[7035+i, 'label'], Y_test[1120+i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Flatten, InputLayer, Bidirectional, concatenate, add, average, Reshape\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把兩邊input concate起來，有加上dropout的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 80, 300)      1967400     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 80, 300)      1967400     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 512)          1665024     embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 512)          1665024     embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1024)         0           lstm_6[0][0]                     \n",
      "                                                                 lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          131200      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 64)           8256        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 3)            195         dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,404,499\n",
      "Trainable params: 7,404,499\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# first input model 1\n",
    "input_layer_1 = Input(shape = (max_seq_length,), dtype='int64')\n",
    "embedding_1 = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], mask_zero=True, trainable=True)(input_layer_1)\n",
    "lstm_hidden_1 = LSTM(512, return_sequences=False, dropout=0.3)(embedding_1) \n",
    "# lstm_hidden_1 = Bidirectional(LSTM(512, return_sequences=False, dropout=0.4))(embedding_1) \n",
    "\n",
    "#second input model 2\n",
    "input_layer_2 = Input(shape = (max_seq_length,), dtype='int64')\n",
    "embedding_2 = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], mask_zero=True, trainable=True)(input_layer_2)\n",
    "lstm_hidden_2 = LSTM(512, return_sequences=False, dropout=0.3)(embedding_2)\n",
    "# lstm_hidden_2 = Bidirectional(LSTM(512, return_sequences=False, dropout=0.4))(embedding_2)\n",
    "\n",
    "#merge input model\n",
    "averaged = concatenate([lstm_hidden_1, lstm_hidden_2])\n",
    "hidden_1 = Dense(128, activation='relu')(averaged)\n",
    "dropout_1 = Dropout(0.3)(hidden_1)\n",
    "hidden_2 = Dense(64, activation='relu')(dropout_1)\n",
    "dropout_2 = Dropout(0.3)(hidden_2)\n",
    "output = Dense(3, activation='softmax')(dropout_2)\n",
    "model1 = Model(inputs=[input_layer_1, input_layer_2], outputs=output)\n",
    "print(model1.summary())\n",
    "adam = Adam(lr=1e-2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model1.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=8, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5915 samples, validate on 1758 samples\n",
      "Epoch 1/30\n",
      "5915/5915 [==============================] - 8s 1ms/sample - loss: 0.9885 - accuracy: 0.5386 - val_loss: 0.9323 - val_accuracy: 0.5848\n",
      "Epoch 2/30\n",
      "5915/5915 [==============================] - 3s 562us/sample - loss: 0.8819 - accuracy: 0.6558 - val_loss: 0.8817 - val_accuracy: 0.6593\n",
      "Epoch 3/30\n",
      "5915/5915 [==============================] - 3s 564us/sample - loss: 0.8109 - accuracy: 0.7351 - val_loss: 0.8813 - val_accuracy: 0.6576\n",
      "Epoch 4/30\n",
      "5915/5915 [==============================] - 3s 570us/sample - loss: 0.7700 - accuracy: 0.7772 - val_loss: 0.8743 - val_accuracy: 0.6701\n",
      "Epoch 5/30\n",
      "5915/5915 [==============================] - 4s 642us/sample - loss: 0.7228 - accuracy: 0.8247 - val_loss: 0.8838 - val_accuracy: 0.6581\n",
      "Epoch 6/30\n",
      "5915/5915 [==============================] - 3s 586us/sample - loss: 0.7150 - accuracy: 0.8331 - val_loss: 0.8620 - val_accuracy: 0.6832\n",
      "Epoch 7/30\n",
      "5915/5915 [==============================] - 3s 572us/sample - loss: 0.6901 - accuracy: 0.8583 - val_loss: 0.8620 - val_accuracy: 0.6826\n",
      "Epoch 8/30\n",
      "5915/5915 [==============================] - 3s 563us/sample - loss: 0.6733 - accuracy: 0.8773 - val_loss: 0.8730 - val_accuracy: 0.6706\n",
      "Epoch 9/30\n",
      "5915/5915 [==============================] - 4s 619us/sample - loss: 0.6640 - accuracy: 0.8857 - val_loss: 0.8554 - val_accuracy: 0.6900\n",
      "Epoch 10/30\n",
      "5915/5915 [==============================] - 3s 573us/sample - loss: 0.6562 - accuracy: 0.8937 - val_loss: 0.8704 - val_accuracy: 0.6718\n",
      "Epoch 11/30\n",
      "5915/5915 [==============================] - 4s 596us/sample - loss: 0.6488 - accuracy: 0.9008 - val_loss: 0.8594 - val_accuracy: 0.6900\n",
      "Epoch 12/30\n",
      "5915/5915 [==============================] - 4s 661us/sample - loss: 0.6427 - accuracy: 0.9075 - val_loss: 0.8458 - val_accuracy: 0.7048\n",
      "Epoch 13/30\n",
      "5915/5915 [==============================] - 3s 564us/sample - loss: 0.6381 - accuracy: 0.9123 - val_loss: 0.8567 - val_accuracy: 0.6860\n",
      "Epoch 14/30\n",
      "5915/5915 [==============================] - 3s 586us/sample - loss: 0.6296 - accuracy: 0.9214 - val_loss: 0.8636 - val_accuracy: 0.6775\n",
      "Epoch 15/30\n",
      "5915/5915 [==============================] - 3s 589us/sample - loss: 0.6227 - accuracy: 0.9287 - val_loss: 0.8549 - val_accuracy: 0.6917\n",
      "Epoch 16/30\n",
      "5915/5915 [==============================] - 4s 658us/sample - loss: 0.6239 - accuracy: 0.9271 - val_loss: 0.8644 - val_accuracy: 0.6837\n",
      "Epoch 17/30\n",
      "5915/5915 [==============================] - 4s 596us/sample - loss: 0.6248 - accuracy: 0.9266 - val_loss: 0.8579 - val_accuracy: 0.6889\n",
      "Epoch 18/30\n",
      "5915/5915 [==============================] - 3s 579us/sample - loss: 0.6182 - accuracy: 0.9320 - val_loss: 0.8526 - val_accuracy: 0.6911\n",
      "Epoch 19/30\n",
      "5915/5915 [==============================] - 4s 650us/sample - loss: 0.6234 - accuracy: 0.9276 - val_loss: 0.8561 - val_accuracy: 0.6917\n",
      "Epoch 20/30\n",
      "5824/5915 [============================>.] - ETA: 0s - loss: 0.6235 - accuracy: 0.9269Restoring model weights from the end of the best epoch.\n",
      "5915/5915 [==============================] - 4s 594us/sample - loss: 0.6238 - accuracy: 0.9265 - val_loss: 0.8601 - val_accuracy: 0.6866\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "model1_fit = model1.fit([X_left_train, X_right_train],Y_train, batch_size=64,epochs=30,\n",
    "                      validation_data=([X_left_test, X_right_test],Y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三元分類還沒argmax output\n",
      "[[5.3300154e-13 2.7310196e-11 1.0000000e+00]\n",
      " [9.9999702e-01 2.6899255e-07 2.7781507e-06]\n",
      " [3.6737475e-15 9.1933655e-13 1.0000000e+00]\n",
      " ...\n",
      " [1.0000000e+00 2.8581860e-13 2.1993028e-13]\n",
      " [3.3382308e-03 9.9622923e-01 4.3247713e-04]\n",
      " [3.3532366e-02 9.9948526e-04 9.6546817e-01]]\n",
      "\n",
      "0.7047781569965871\n",
      "[[185  32 107]\n",
      " [ 94 118 153]\n",
      " [ 66  67 936]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.57      0.55       324\n",
      "           1       0.54      0.32      0.41       365\n",
      "           2       0.78      0.88      0.83      1069\n",
      "\n",
      "    accuracy                           0.70      1758\n",
      "   macro avg       0.62      0.59      0.60      1758\n",
      "weighted avg       0.69      0.70      0.69      1758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 取所有label中的test label\n",
    "Y_label = data['label'].to_numpy()[5915:]\n",
    "\n",
    "# test data confusion matrix\n",
    "predictions= model1.predict([X_left_test, X_right_test]) # 輸出的是n*5的編碼值array\n",
    "print('三元分類還沒argmax output')\n",
    "print(predictions)\n",
    "print()\n",
    "predictions = np.argmax(predictions, axis=1) # axis = 1是取行的最大值的索引，0是列的最大值的索引\n",
    "print(accuracy_score(Y_label, predictions))\n",
    "print(confusion_matrix(Y_label, predictions))\n",
    "print(classification_report(Y_label, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7383928571428572\n",
      "[[107  17  72]\n",
      " [ 39  61  96]\n",
      " [ 35  34 659]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.55      0.57       196\n",
      "           1       0.54      0.31      0.40       196\n",
      "           2       0.80      0.91      0.85       728\n",
      "\n",
      "    accuracy                           0.74      1120\n",
      "   macro avg       0.64      0.59      0.60      1120\n",
      "weighted avg       0.72      0.74      0.72      1120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 取所有label中的laptop test lable\n",
    "laptop_label = data['label'].to_numpy()[5915:7035]\n",
    "\n",
    "# laptop test data confusion matrix\n",
    "predictions= model1.predict([X_left_test[:1120], X_right_test[:1120]]) # 輸出的是n*5的編碼值array\n",
    "predictions = np.argmax(predictions, axis=1) # axis = 1是取行的最大值的索引，0是列的最大值的索引\n",
    "print(accuracy_score(laptop_label, predictions))\n",
    "print(confusion_matrix(laptop_label, predictions))\n",
    "print(classification_report(laptop_label, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64576802507837\n",
      "[[ 78  15  35]\n",
      " [ 55  57  57]\n",
      " [ 31  33 277]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.61      0.53       128\n",
      "           1       0.54      0.34      0.42       169\n",
      "           2       0.75      0.81      0.78       341\n",
      "\n",
      "    accuracy                           0.65       638\n",
      "   macro avg       0.59      0.59      0.58       638\n",
      "weighted avg       0.64      0.65      0.63       638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 取所有label中的restaurant test lable\n",
    "restaurant_label = data['label'].to_numpy()[7035:]\n",
    "\n",
    "# restaurant test data confusion matrix\n",
    "predictions= model1.predict([X_left_test[1120:], X_right_test[1120:]]) # 輸出的是n*5的編碼值array\n",
    "predictions = np.argmax(predictions, axis=1) # axis = 1是取行的最大值的索引，0是列的最大值的索引\n",
    "print(accuracy_score(restaurant_label, predictions))\n",
    "print(confusion_matrix(restaurant_label, predictions))\n",
    "print(classification_report(restaurant_label, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get train、test LSTM 64 dimension output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output=model1.get_layer('dense_9').output\n",
    "intermediate_model = Model(inputs=[input_layer_1, input_layer_2],outputs=layer_output)\n",
    "intermediate_prediction=intermediate_model.predict([left_text_seq, right_text_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(7673, 128)\n",
      "[[0.         0.         0.         ... 0.         1.409985   2.460079  ]\n",
      " [0.42525083 0.17294298 1.302797   ... 0.         0.         0.        ]\n",
      " [0.4482868  0.         1.1084592  ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         1.712281   2.9153383 ]\n",
      " [0.6232571  0.8368558  0.         ... 0.22482945 0.         0.2144836 ]\n",
      " [0.         0.         0.0696213  ... 0.         0.6006517  0.07825752]]\n"
     ]
    }
   ],
   "source": [
    "print(type(intermediate_prediction))\n",
    "print(intermediate_prediction.shape)\n",
    "print(intermediate_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把LSTM dimension 放進dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/bertenv3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspect</th>\n",
       "      <th>polarity</th>\n",
       "      <th>left_text</th>\n",
       "      <th>right_text</th>\n",
       "      <th>left_right_text</th>\n",
       "      <th>label</th>\n",
       "      <th>lstm_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7663</th>\n",
       "      <td>also...- excellent operating system- size and ...</td>\n",
       "      <td>weight</td>\n",
       "      <td>positive</td>\n",
       "      <td>also  excellent operating system  size and weight</td>\n",
       "      <td>weight for optimal mobility  excellent durabil...</td>\n",
       "      <td>also  excellent operating system  size and wei...</td>\n",
       "      <td>2</td>\n",
       "      <td>[1.442924, 0.118619904, 1.0683073, 1.8290948, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7664</th>\n",
       "      <td>also...- excellent operating system- size and ...</td>\n",
       "      <td>mobility</td>\n",
       "      <td>positive</td>\n",
       "      <td>also  excellent operating system  size and wei...</td>\n",
       "      <td>mobility  excellent durability of the battery ...</td>\n",
       "      <td>also  excellent operating system  size and wei...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.64442265, 0.4648288, 0.8578217, 1.8430195, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7665</th>\n",
       "      <td>also...- excellent operating system- size and ...</td>\n",
       "      <td>durability of the battery</td>\n",
       "      <td>positive</td>\n",
       "      <td>also  excellent operating system  size and wei...</td>\n",
       "      <td>durability of the battery  the functions provi...</td>\n",
       "      <td>also  excellent operating system  size and wei...</td>\n",
       "      <td>2</td>\n",
       "      <td>[1.1156328, 0.0, 1.1949103, 1.7904588, 0.56774...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7666</th>\n",
       "      <td>also...- excellent operating system- size and ...</td>\n",
       "      <td>functions provided by the trackpad</td>\n",
       "      <td>positive</td>\n",
       "      <td>also  excellent operating system  size and wei...</td>\n",
       "      <td>functions provided by the trackpad is unmatche...</td>\n",
       "      <td>also  excellent operating system  size and wei...</td>\n",
       "      <td>2</td>\n",
       "      <td>[1.6156456, 0.0, 1.5932965, 2.4519243, 0.50335...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7667</th>\n",
       "      <td>This hardware seems to be better than the iMac...</td>\n",
       "      <td>hardware</td>\n",
       "      <td>positive</td>\n",
       "      <td>this hardware</td>\n",
       "      <td>hardware seems to be better than the imac in t...</td>\n",
       "      <td>this hardware hardware seems to be better than...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.26618752, 0.0, 0.0, 0.0, 0.0, 0.0, 0.803694...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7668</th>\n",
       "      <td>I've had it for about 2 months now and found n...</td>\n",
       "      <td>software</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i've had it for about 2 months now and found n...</td>\n",
       "      <td>software or updates</td>\n",
       "      <td>i've had it for about 2 months now and found n...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.515463, 0.0, 0.0, 0.0, 0.0, 0.7722197,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7669</th>\n",
       "      <td>I've had it for about 2 months now and found n...</td>\n",
       "      <td>updates</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i've had it for about 2 months now and found n...</td>\n",
       "      <td>updates</td>\n",
       "      <td>i've had it for about 2 months now and found n...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.11405362, 0.0, 0.0, 0.0, 0.0, 0.975277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7670</th>\n",
       "      <td>the latest version does not have a disc drive.</td>\n",
       "      <td>disc drive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>the latest version does not have a disc drive</td>\n",
       "      <td>disc drive</td>\n",
       "      <td>the latest version does not have a disc drive ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2103522, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7671</th>\n",
       "      <td>Screen - although some people might complain a...</td>\n",
       "      <td>Screen</td>\n",
       "      <td>positive</td>\n",
       "      <td>screen</td>\n",
       "      <td>screen   although some people might complain a...</td>\n",
       "      <td>screen screen   although some people might com...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.6232571, 0.8368558, 0.0, 0.0, 0.29151317, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7672</th>\n",
       "      <td>Screen - although some people might complain a...</td>\n",
       "      <td>res</td>\n",
       "      <td>positive</td>\n",
       "      <td>screen   although some people might complain a...</td>\n",
       "      <td>res which i think is ridiculous</td>\n",
       "      <td>screen   although some people might complain a...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 0.0696213, 0.177733, 0.0, 0.6158762...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "7663  also...- excellent operating system- size and ...   \n",
       "7664  also...- excellent operating system- size and ...   \n",
       "7665  also...- excellent operating system- size and ...   \n",
       "7666  also...- excellent operating system- size and ...   \n",
       "7667  This hardware seems to be better than the iMac...   \n",
       "7668  I've had it for about 2 months now and found n...   \n",
       "7669  I've had it for about 2 months now and found n...   \n",
       "7670     the latest version does not have a disc drive.   \n",
       "7671  Screen - although some people might complain a...   \n",
       "7672  Screen - although some people might complain a...   \n",
       "\n",
       "                                  aspect  polarity  \\\n",
       "7663                              weight  positive   \n",
       "7664                            mobility  positive   \n",
       "7665           durability of the battery  positive   \n",
       "7666  functions provided by the trackpad  positive   \n",
       "7667                            hardware  positive   \n",
       "7668                            software   neutral   \n",
       "7669                             updates   neutral   \n",
       "7670                          disc drive   neutral   \n",
       "7671                              Screen  positive   \n",
       "7672                                 res  positive   \n",
       "\n",
       "                                              left_text  \\\n",
       "7663  also  excellent operating system  size and weight   \n",
       "7664  also  excellent operating system  size and wei...   \n",
       "7665  also  excellent operating system  size and wei...   \n",
       "7666  also  excellent operating system  size and wei...   \n",
       "7667                                      this hardware   \n",
       "7668  i've had it for about 2 months now and found n...   \n",
       "7669  i've had it for about 2 months now and found n...   \n",
       "7670      the latest version does not have a disc drive   \n",
       "7671                                             screen   \n",
       "7672  screen   although some people might complain a...   \n",
       "\n",
       "                                             right_text  \\\n",
       "7663  weight for optimal mobility  excellent durabil...   \n",
       "7664  mobility  excellent durability of the battery ...   \n",
       "7665  durability of the battery  the functions provi...   \n",
       "7666  functions provided by the trackpad is unmatche...   \n",
       "7667  hardware seems to be better than the imac in t...   \n",
       "7668                                software or updates   \n",
       "7669                                            updates   \n",
       "7670                                         disc drive   \n",
       "7671  screen   although some people might complain a...   \n",
       "7672                    res which i think is ridiculous   \n",
       "\n",
       "                                        left_right_text  label  \\\n",
       "7663  also  excellent operating system  size and wei...      2   \n",
       "7664  also  excellent operating system  size and wei...      2   \n",
       "7665  also  excellent operating system  size and wei...      2   \n",
       "7666  also  excellent operating system  size and wei...      2   \n",
       "7667  this hardware hardware seems to be better than...      2   \n",
       "7668  i've had it for about 2 months now and found n...      1   \n",
       "7669  i've had it for about 2 months now and found n...      1   \n",
       "7670  the latest version does not have a disc drive ...      1   \n",
       "7671  screen screen   although some people might com...      2   \n",
       "7672  screen   although some people might complain a...      2   \n",
       "\n",
       "                                           lstm_predict  \n",
       "7663  [1.442924, 0.118619904, 1.0683073, 1.8290948, ...  \n",
       "7664  [0.64442265, 0.4648288, 0.8578217, 1.8430195, ...  \n",
       "7665  [1.1156328, 0.0, 1.1949103, 1.7904588, 0.56774...  \n",
       "7666  [1.6156456, 0.0, 1.5932965, 2.4519243, 0.50335...  \n",
       "7667  [0.26618752, 0.0, 0.0, 0.0, 0.0, 0.0, 0.803694...  \n",
       "7668  [0.0, 0.515463, 0.0, 0.0, 0.0, 0.0, 0.7722197,...  \n",
       "7669  [0.0, 0.11405362, 0.0, 0.0, 0.0, 0.0, 0.975277...  \n",
       "7670  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2103522, 0.0,...  \n",
       "7671  [0.6232571, 0.8368558, 0.0, 0.0, 0.29151317, 0...  \n",
       "7672  [0.0, 0.0, 0.0696213, 0.177733, 0.0, 0.6158762...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把LSTM predict出來的東西放進dataframe\n",
    "data['lstm_predict'] = 'N/A'\n",
    "for i in range(len(data)):\n",
    "    data['lstm_predict'][i] = intermediate_prediction[i]\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "128\n",
      "[0.44748157 0.         0.8662199  1.6753371  0.         1.5549994\n",
      " 0.         1.3884728  0.         0.         0.         0.8880838\n",
      " 0.         0.9054352  0.         0.10345642 0.         0.18495762\n",
      " 0.24724263 0.16130812 0.         2.325821   1.718151   1.7964698\n",
      " 0.         0.2915889  0.         1.7786036  0.525636   0.\n",
      " 0.8316277  1.1500238  0.0913147  0.         0.         0.\n",
      " 0.         0.         0.         1.3722969  0.81445724 0.\n",
      " 0.         0.         0.         0.6655453  1.3237424  0.\n",
      " 0.         1.766793   0.00810153 0.         0.         0.\n",
      " 0.0910603  0.         0.         0.         0.         1.6202874\n",
      " 0.         0.         0.         1.0358601  0.         0.2098353\n",
      " 0.         0.         0.         0.         0.07523081 0.\n",
      " 0.         0.27667302 0.         0.         1.3327559  1.1369873\n",
      " 0.         0.         1.4422199  0.         0.         0.9922794\n",
      " 1.2358521  0.         0.         1.7379907  0.6183897  0.46170944\n",
      " 1.5591501  0.         0.         0.         1.8908706  0.\n",
      " 0.         0.         1.4591899  0.00344026 0.         0.25571728\n",
      " 0.         0.         0.02903458 1.4510349  0.         1.3814934\n",
      " 0.09978106 1.6528093  0.         2.425191   0.         0.\n",
      " 0.         0.         1.6850119  1.6105855  0.         0.\n",
      " 0.         1.4818256  1.5758667  0.         0.         0.\n",
      " 0.33444053 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# 稽查\n",
    "n = 504\n",
    "print(data.loc[n, 'lstm_predict'].shape)\n",
    "print(len(data.loc[n, 'lstm_predict']))\n",
    "print(data.loc[n, 'lstm_predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, TFBertForSequenceClassification, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer, to convert our text into tokens that correspond to BERT’s vocabulary.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 找出單句最多token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出最多text add aspect中最多是幾個token，不包含CLS跟SEP\n",
    "def find_max_token(pd):\n",
    "    max_token = 0\n",
    "    index = 0\n",
    "    for i in range(len(pd)):\n",
    "        text = pd.loc[i, 'text']\n",
    "        aspect = pd.loc[i, 'aspect']\n",
    "        text_aspect = text + ' ' +aspect\n",
    "        tokens_len = len(tokenizer.tokenize(text_aspect))\n",
    "        if tokens_len>max_token:\n",
    "            max_token = tokens_len\n",
    "            index = i\n",
    "    return [max_token, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料集token最多與index是: [99, 7419]\n"
     ]
    }
   ],
   "source": [
    "# 找出text add aspect中token最多的是幾個token，不包含CLS跟SEP\n",
    "max_token = find_max_token(data)\n",
    "print('資料集token最多與index是:', max_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正式把資料轉換成token(padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 把句子轉變成token(CLS+text+SEP+asepct)+(padding)的function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把維度固定在128維\n",
    "input_dim = 128\n",
    "def input_ids_all(pd):\n",
    "    pd['input_ids'] = 'N/A'\n",
    "    for i in range(len(pd)):\n",
    "        text = pd.loc[i, 'text']\n",
    "        aspect = pd.loc[i, 'aspect']\n",
    "        text_tokens = tokenizer.tokenize(text) # 把text轉成token\n",
    "        aspect_tokens = tokenizer.tokenize(aspect) # 把aspect轉成token\n",
    "        \n",
    "        text_input_ids = tokenizer.convert_tokens_to_ids(text_tokens) # 把text token轉成text token id\n",
    "        aspect_input_ids = tokenizer.convert_tokens_to_ids(aspect_tokens) # 把aspect token轉成aspect token id\n",
    "        \n",
    "        text_input_ids_cls = tokenizer.build_inputs_with_special_tokens(text_input_ids) # aspect token id加上CLS、SEP token id\n",
    "        input_ids = text_input_ids_cls + aspect_input_ids # 把aspect token id接在text token id 後面 (CLS+text+SEP+aspect)\n",
    "        input_ids = np.array(input_ids)\n",
    "        \n",
    "        if len(input_ids) < input_dim:\n",
    "            n = input_dim - len(input_ids)\n",
    "            input_ids = np.pad(input_ids, (0, n), mode ='constant', constant_values=(0)) # array右邊append n 個 0  補長度到512\n",
    "        \n",
    "        pd['input_ids'][i] = input_ids\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/bertenv3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspect</th>\n",
       "      <th>polarity</th>\n",
       "      <th>left_text</th>\n",
       "      <th>right_text</th>\n",
       "      <th>left_right_text</th>\n",
       "      <th>label</th>\n",
       "      <th>lstm_predict</th>\n",
       "      <th>input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "      <td>but the staff</td>\n",
       "      <td>staff was so horrible to us</td>\n",
       "      <td>but the staff staff was so horrible to us</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.3246815, 0.0,...</td>\n",
       "      <td>[101, 2021, 1996, 3095, 2001, 2061, 9202, 2000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>to be completely fair the only redeeming facto...</td>\n",
       "      <td>food which was above average but couldn't make...</td>\n",
       "      <td>to be completely fair the only redeeming facto...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.42525083, 0.17294298, 1.302797, 2.6707382, ...</td>\n",
       "      <td>[101, 2000, 2022, 3294, 4189, 1010, 1996, 2069...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>the food</td>\n",
       "      <td>food is uniformly exceptional with a very capa...</td>\n",
       "      <td>the food food is uniformly exceptional with a ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.4482868, 0.0, 1.1084592, 2.1217976, 0.08224...</td>\n",
       "      <td>[101, 1996, 2833, 2003, 27423, 11813, 1010, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>kitchen which will proudly whip up whatever yo...</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.654872, 0.0, 1.642791, 2.8826728, 0.0226560...</td>\n",
       "      <td>[101, 1996, 2833, 2003, 27423, 11813, 1010, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>menu or not</td>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.0776782, 1.8292367, 0.0, 0.0, 1.2995703, 0....</td>\n",
       "      <td>[101, 1996, 2833, 2003, 27423, 11813, 1010, 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   aspect  polarity  \\\n",
       "0               But the staff was so horrible to us.    staff  negative   \n",
       "1  To be completely fair, the only redeeming fact...     food  positive   \n",
       "2  The food is uniformly exceptional, with a very...     food  positive   \n",
       "3  The food is uniformly exceptional, with a very...  kitchen  positive   \n",
       "4  The food is uniformly exceptional, with a very...     menu   neutral   \n",
       "\n",
       "                                           left_text  \\\n",
       "0                                      but the staff   \n",
       "1  to be completely fair the only redeeming facto...   \n",
       "2                                           the food   \n",
       "3  the food is uniformly exceptional with a very ...   \n",
       "4  the food is uniformly exceptional with a very ...   \n",
       "\n",
       "                                          right_text  \\\n",
       "0                        staff was so horrible to us   \n",
       "1  food which was above average but couldn't make...   \n",
       "2  food is uniformly exceptional with a very capa...   \n",
       "3  kitchen which will proudly whip up whatever yo...   \n",
       "4                                        menu or not   \n",
       "\n",
       "                                     left_right_text  label  \\\n",
       "0          but the staff staff was so horrible to us      0   \n",
       "1  to be completely fair the only redeeming facto...      2   \n",
       "2  the food food is uniformly exceptional with a ...      2   \n",
       "3  the food is uniformly exceptional with a very ...      2   \n",
       "4  the food is uniformly exceptional with a very ...      1   \n",
       "\n",
       "                                        lstm_predict  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.3246815, 0.0,...   \n",
       "1  [0.42525083, 0.17294298, 1.302797, 2.6707382, ...   \n",
       "2  [0.4482868, 0.0, 1.1084592, 2.1217976, 0.08224...   \n",
       "3  [0.654872, 0.0, 1.642791, 2.8826728, 0.0226560...   \n",
       "4  [1.0776782, 1.8292367, 0.0, 0.0, 1.2995703, 0....   \n",
       "\n",
       "                                           input_ids  \n",
       "0  [101, 2021, 1996, 3095, 2001, 2061, 9202, 2000...  \n",
       "1  [101, 2000, 2022, 3294, 4189, 1010, 1996, 2069...  \n",
       "2  [101, 1996, 2833, 2003, 27423, 11813, 1010, 20...  \n",
       "3  [101, 1996, 2833, 2003, 27423, 11813, 1010, 20...  \n",
       "4  [101, 1996, 2833, 2003, 27423, 11813, 1010, 20...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將text轉成token，後面加上aspect token存進dataframe\n",
    "data = input_ids_all(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skillfully chosen Portuguese cheese cart paired with quality port provides the perfect Iberian ending.\n",
      "port\n",
      "[  101  1996  8066  7699  4217  5077  8808 11122 12739  2007  3737  3417\n",
      "  3640  1996  3819 21988  4566  1012   102  3417     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# 稽查\n",
    "n = 6299\n",
    "print(data.loc[n, 'text'])\n",
    "print(data.loc[n, 'aspect'])\n",
    "print(data.loc[n, 'input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7673, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 101, 2021, 1996, ...,    0,    0,    0],\n",
       "       [ 101, 2000, 2022, ...,    0,    0,    0],\n",
       "       [ 101, 1996, 2833, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 1996, 6745, ...,    0,    0,    0],\n",
       "       [ 101, 3898, 1011, ...,    0,    0,    0],\n",
       "       [ 101, 3898, 1011, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把data的input_ids提出存進list\n",
    "input_ids = list()\n",
    "for i in range(len(data)):\n",
    "    np_id = data.loc[i, 'input_ids']\n",
    "    input_ids.append(np_id)\n",
    "input_ids = np.array(input_ids)\n",
    "print(input_ids.shape)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(7673, 128)\n",
      "[[0.         0.         0.         ... 0.         1.409985   2.460079  ]\n",
      " [0.42525083 0.17294298 1.302797   ... 0.         0.         0.        ]\n",
      " [0.4482868  0.         1.1084592  ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         1.712281   2.9153383 ]\n",
      " [0.6232571  0.8368558  0.         ... 0.22482945 0.         0.2144836 ]\n",
      " [0.         0.         0.0696213  ... 0.         0.6006517  0.07825752]]\n"
     ]
    }
   ],
   "source": [
    "# 把data的lstm_predcit提出存進list\n",
    "lstm_predict = list()\n",
    "for i in range(len(data)):\n",
    "    np_lstm = data.loc[i, 'lstm_predict']\n",
    "    lstm_predict.append(np_lstm)\n",
    "lstm_predict = np.array(lstm_predict)\n",
    "print(type(lstm_predict))\n",
    "print(lstm_predict.shape)\n",
    "print(lstm_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, ..., 1, 2, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把data label變成numpy\n",
    "label = data['label'].to_numpy()\n",
    "print(len(label))\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切train、test資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5915, 128)\n",
      "[[ 101 2021 1996 ...    0    0    0]\n",
      " [ 101 2000 2022 ...    0    0    0]\n",
      " [ 101 1996 2833 ...    0    0    0]\n",
      " ...\n",
      " [ 101 2057 2036 ...    0    0    0]\n",
      " [ 101 2129 2000 ...    0    0    0]\n",
      " [ 101 1045 2052 ...    0    0    0]]\n",
      "\n",
      "(1758, 128)\n",
      "[[ 101 1996 7852 ...    0    0    0]\n",
      " [ 101 1045 2031 ...    0    0    0]\n",
      " [ 101 2833 2003 ...    0    0    0]\n",
      " ...\n",
      " [ 101 1996 6745 ...    0    0    0]\n",
      " [ 101 3898 1011 ...    0    0    0]\n",
      " [ 101 3898 1011 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# X\n",
    "train_input_ids = input_ids[:5915]\n",
    "test_input_ids = input_ids[5915:]\n",
    "print(train_input_ids.shape)\n",
    "print(train_input_ids)\n",
    "print()\n",
    "print(test_input_ids.shape)\n",
    "print(test_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5915, 128)\n",
      "(1758, 128)\n"
     ]
    }
   ],
   "source": [
    "# lstm predict\n",
    "train_lstm_predict = lstm_predict[:5915]\n",
    "test_lstm_predict = lstm_predict[5915:]\n",
    "print(train_lstm_predict.shape)\n",
    "print(test_lstm_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5915,)\n",
      "[0 2 2 ... 1 2 1]\n",
      "\n",
      "(1758,)\n",
      "[2 2 2 ... 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Y\n",
    "train_label = label[:5915]\n",
    "test_label = label[5915:]\n",
    "print(train_label.shape)\n",
    "print(train_label)\n",
    "print()\n",
    "print(test_label.shape)\n",
    "print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data\n",
      "positive 2 2\n",
      "positive 2 2\n",
      "positive 2 2\n",
      "positive 2 2\n",
      "positive 2 2\n",
      "positive 2 2\n",
      "positive 2 2\n",
      "positive 2 2\n",
      "positive 2 2\n",
      "positive 2 2\n",
      "neutral 1 1\n",
      "positive 2 2\n",
      "positive 2 2\n",
      "positive 2 2\n",
      "negative 0 0\n"
     ]
    }
   ],
   "source": [
    "# 檢查polarity跟label有沒有不一樣\n",
    "print('test_data')\n",
    "for i in range(15):\n",
    "    print(test_data.loc[i, 'polarity'], data.loc[5915+i, 'label'], test_label[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 128, 768)     0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 98304)        0           dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 512)          50332160    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 640)          0           dense_12[0][0]                   \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 3)            1923        concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 159,816,323\n",
      "Trainable params: 159,816,323\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer= Input(shape = (128,), dtype='int64')\n",
    "lstm_input_layer = Input(shape = (128,), dtype='float32')\n",
    "print(type(input_layer))\n",
    "print(type(lstm_input_layer))\n",
    "bert = TFBertModel.from_pretrained('bert-base-uncased')(input_layer)\n",
    "bert = bert[0]\n",
    "dropout = Dropout(0.1)(bert)\n",
    "flat = Flatten()(dropout)\n",
    "dense_1 = Dense(units=512)(flat)\n",
    "print(type(dense_1))\n",
    "\n",
    "merge = concatenate([dense_1, lstm_input_layer])\n",
    "classifier = Dense(units=3)(merge) # 分3類\n",
    "model = Model(inputs=[input_layer, lstm_input_layer], outputs=classifier)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule \n",
    "# # num_labels=3 分3類\n",
    "# model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "# model.summary()\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5915 samples, validate on 1758 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "5915/5915 [==============================] - 138s 23ms/sample - loss: 0.6811 - accuracy: 0.7430 - val_loss: 0.6712 - val_accuracy: 0.7395\n",
      "Epoch 2/5\n",
      "5915/5915 [==============================] - 127s 22ms/sample - loss: 0.3445 - accuracy: 0.8712 - val_loss: 0.5076 - val_accuracy: 0.8055\n",
      "Epoch 3/5\n",
      "5915/5915 [==============================] - 125s 21ms/sample - loss: 0.2173 - accuracy: 0.9261 - val_loss: 0.6906 - val_accuracy: 0.7759\n",
      "Epoch 4/5\n",
      "5915/5915 [==============================] - 128s 22ms/sample - loss: 0.1664 - accuracy: 0.9386 - val_loss: 0.7413 - val_accuracy: 0.7742\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit([train_input_ids,train_lstm_predict], train_label, \n",
    "                      batch_size=4, epochs=5, \n",
    "                      validation_data=([test_input_ids, test_lstm_predict], test_label), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8054607508532423\n",
      "[[208  81  35]\n",
      " [ 47 257  61]\n",
      " [ 27  91 951]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.64      0.69       324\n",
      "           1       0.60      0.70      0.65       365\n",
      "           2       0.91      0.89      0.90      1069\n",
      "\n",
      "    accuracy                           0.81      1758\n",
      "   macro avg       0.75      0.75      0.74      1758\n",
      "weighted avg       0.81      0.81      0.81      1758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test data confusion\n",
    "predictions_test= model.predict([test_input_ids,test_lstm_predict]) # 輸出的是n*5的編碼值array\n",
    "predictions_test = np.argmax(predictions_test, axis=1) # axis = 1是取行的最大值的索引，0是列的最大值的索引\n",
    "print(accuracy_score(test_label, predictions_test))\n",
    "print(confusion_matrix(test_label, predictions_test))\n",
    "print(classification_report(test_label, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8196428571428571\n",
      "[[121  51  24]\n",
      " [ 19 137  40]\n",
      " [ 14  54 660]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.62      0.69       196\n",
      "           1       0.57      0.70      0.63       196\n",
      "           2       0.91      0.91      0.91       728\n",
      "\n",
      "    accuracy                           0.82      1120\n",
      "   macro avg       0.75      0.74      0.74      1120\n",
      "weighted avg       0.83      0.82      0.82      1120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# restaurant_test confusion\n",
    "laptop_test_input_ids = test_input_ids[:1120]\n",
    "laptop_test_lstm_predict = test_lstm_predict[:1120]\n",
    "laptop_test_label = test_label[:1120]\n",
    "predictions_lap_test = model.predict([laptop_test_input_ids, laptop_test_lstm_predict])\n",
    "predictions_lap_test = np.argmax(predictions_lap_test, axis=1) # axis = 1是取行的最大值的索引，0是列的最大值的索引\n",
    "print(accuracy_score(laptop_test_label, predictions_lap_test))\n",
    "print(confusion_matrix(laptop_test_label, predictions_lap_test))\n",
    "print(classification_report(laptop_test_label, predictions_lap_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.780564263322884\n",
      "[[ 87  30  11]\n",
      " [ 28 120  21]\n",
      " [ 13  37 291]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       128\n",
      "           1       0.64      0.71      0.67       169\n",
      "           2       0.90      0.85      0.88       341\n",
      "\n",
      "    accuracy                           0.78       638\n",
      "   macro avg       0.74      0.75      0.74       638\n",
      "weighted avg       0.79      0.78      0.78       638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# laptop_test confusion\n",
    "restaurant_test_input_ids = test_input_ids[1120:]\n",
    "restaurant_test_label = test_label[1120:]\n",
    "restaurant_test_lstm_predict = test_lstm_predict[1120:]\n",
    "predictions_res_test = model.predict([restaurant_test_input_ids, restaurant_test_lstm_predict])\n",
    "predictions_res_test = np.argmax(predictions_res_test, axis=1) # axis = 1是取行的最大值的索引，0是列的最大值的索引\n",
    "print(accuracy_score(restaurant_test_label, predictions_res_test))\n",
    "print(confusion_matrix(restaurant_test_label, predictions_res_test))\n",
    "print(classification_report(restaurant_test_label, predictions_res_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
