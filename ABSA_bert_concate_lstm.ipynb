{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用bert預測句子(CLS+text+SEP+asepct)的ht，concate LSTM的ht，最後再進行分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow keras跟tfbert一起跑會出問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 對處理好的laptop、restaurant的train、test資料作LSTM前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把dataframe裡的text切成text左邊跟右邊並做一些處理的function\n",
    "def split_text(df):\n",
    "    df['left_text'] = 'N/A'\n",
    "    df['right_text'] = 'N/A'\n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        text = df.loc[i, 'text']\n",
    "        aspect = df.loc[i, 'aspect']\n",
    "        text_split = text.split(aspect) # 根據aspect切割text左右邊\n",
    "        \n",
    "        left_text = text_split[0]+aspect\n",
    "        right_text = aspect+text_split[1]\n",
    "        left_text = left_text.lower() # 把字串變成小寫\n",
    "        right_text = right_text.lower()\n",
    "        left_text = re.sub('-', ' ', left_text)\n",
    "        right_text = re.sub('-', ' ', right_text)\n",
    "        left_text = re.sub('[.,!\"()#%&/:?~]', '', left_text) # 把字串中的一些符號刪除\n",
    "        right_text = re.sub('[.,!\"()#%&/:?~]', '', right_text)\n",
    "        \n",
    "        df.loc[i,'left_text'] = left_text\n",
    "        df.loc[i,'right_text'] = right_text\n",
    "        df.loc[i, 'left_right_text'] = left_text +' '+ right_text # 用來文字encoding\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7673/7673 [00:02<00:00, 2786.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練資料集: 5915\n",
      "測試資料集: 1758\n",
      "所有資料集: 7673\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspect</th>\n",
       "      <th>polarity</th>\n",
       "      <th>left_text</th>\n",
       "      <th>right_text</th>\n",
       "      <th>left_right_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>cord</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i charge it at night and skip taking the cord</td>\n",
       "      <td>cord with me because of the good battery life</td>\n",
       "      <td>i charge it at night and skip taking the cord ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>battery life</td>\n",
       "      <td>positive</td>\n",
       "      <td>i charge it at night and skip taking the cord ...</td>\n",
       "      <td>battery life</td>\n",
       "      <td>i charge it at night and skip taking the cord ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>service center</td>\n",
       "      <td>negative</td>\n",
       "      <td>the tech guy then said the service center</td>\n",
       "      <td>service center does not do 1 to 1 exchange and...</td>\n",
       "      <td>the tech guy then said the service center serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>\"sales\" team</td>\n",
       "      <td>negative</td>\n",
       "      <td>the tech guy then said the service center does...</td>\n",
       "      <td>sales team which is the retail shop which i bo...</td>\n",
       "      <td>the tech guy then said the service center does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>tech guy</td>\n",
       "      <td>neutral</td>\n",
       "      <td>the tech guy</td>\n",
       "      <td>tech guy then said the service center does not...</td>\n",
       "      <td>the tech guy tech guy then said the service ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it is of high quality, has a killer GUI, is ex...</td>\n",
       "      <td>quality</td>\n",
       "      <td>positive</td>\n",
       "      <td>it is of high quality</td>\n",
       "      <td>quality has a killer gui is extremely stable i...</td>\n",
       "      <td>it is of high quality quality has a killer gui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>it is of high quality, has a killer GUI, is ex...</td>\n",
       "      <td>GUI</td>\n",
       "      <td>positive</td>\n",
       "      <td>it is of high quality has a killer gui</td>\n",
       "      <td>gui is extremely stable is highly expandable i...</td>\n",
       "      <td>it is of high quality has a killer gui gui is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>it is of high quality, has a killer GUI, is ex...</td>\n",
       "      <td>applications</td>\n",
       "      <td>positive</td>\n",
       "      <td>it is of high quality has a killer gui is extr...</td>\n",
       "      <td>applications is easy to use and is absolutely ...</td>\n",
       "      <td>it is of high quality has a killer gui is extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>it is of high quality, has a killer GUI, is ex...</td>\n",
       "      <td>use</td>\n",
       "      <td>positive</td>\n",
       "      <td>it is of high quality has a killer gui is extr...</td>\n",
       "      <td>use and is absolutely gorgeous</td>\n",
       "      <td>it is of high quality has a killer gui is extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Easy to start up and does not overheat as much...</td>\n",
       "      <td>start up</td>\n",
       "      <td>positive</td>\n",
       "      <td>easy to start up</td>\n",
       "      <td>start up and does not overheat as much as othe...</td>\n",
       "      <td>easy to start up start up and does not overhea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          aspect  \\\n",
       "0  I charge it at night and skip taking the cord ...            cord   \n",
       "1  I charge it at night and skip taking the cord ...    battery life   \n",
       "2  The tech guy then said the service center does...  service center   \n",
       "3  The tech guy then said the service center does...    \"sales\" team   \n",
       "4  The tech guy then said the service center does...        tech guy   \n",
       "5  it is of high quality, has a killer GUI, is ex...         quality   \n",
       "6  it is of high quality, has a killer GUI, is ex...             GUI   \n",
       "7  it is of high quality, has a killer GUI, is ex...    applications   \n",
       "8  it is of high quality, has a killer GUI, is ex...             use   \n",
       "9  Easy to start up and does not overheat as much...        start up   \n",
       "\n",
       "   polarity                                          left_text  \\\n",
       "0   neutral      i charge it at night and skip taking the cord   \n",
       "1  positive  i charge it at night and skip taking the cord ...   \n",
       "2  negative          the tech guy then said the service center   \n",
       "3  negative  the tech guy then said the service center does...   \n",
       "4   neutral                                       the tech guy   \n",
       "5  positive                              it is of high quality   \n",
       "6  positive             it is of high quality has a killer gui   \n",
       "7  positive  it is of high quality has a killer gui is extr...   \n",
       "8  positive  it is of high quality has a killer gui is extr...   \n",
       "9  positive                                   easy to start up   \n",
       "\n",
       "                                          right_text  \\\n",
       "0      cord with me because of the good battery life   \n",
       "1                                       battery life   \n",
       "2  service center does not do 1 to 1 exchange and...   \n",
       "3  sales team which is the retail shop which i bo...   \n",
       "4  tech guy then said the service center does not...   \n",
       "5  quality has a killer gui is extremely stable i...   \n",
       "6  gui is extremely stable is highly expandable i...   \n",
       "7  applications is easy to use and is absolutely ...   \n",
       "8                     use and is absolutely gorgeous   \n",
       "9  start up and does not overheat as much as othe...   \n",
       "\n",
       "                                     left_right_text  \n",
       "0  i charge it at night and skip taking the cord ...  \n",
       "1  i charge it at night and skip taking the cord ...  \n",
       "2  the tech guy then said the service center serv...  \n",
       "3  the tech guy then said the service center does...  \n",
       "4  the tech guy tech guy then said the service ce...  \n",
       "5  it is of high quality quality has a killer gui...  \n",
       "6  it is of high quality has a killer gui gui is ...  \n",
       "7  it is of high quality has a killer gui is extr...  \n",
       "8  it is of high quality has a killer gui is extr...  \n",
       "9  easy to start up start up and does not overhea...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_train = pd.read_csv('dataset/laptop_train_processed.csv', encoding='utf-8')\n",
    "restaurant_train = pd.read_csv('dataset/restaurant_train_processed.csv', encoding='utf-8')\n",
    "laptop_test = pd.read_csv('dataset/laptop_test_processed.csv', encoding='utf-8')\n",
    "restaurant_test = pd.read_csv('dataset/restaurant_test_processed.csv', encoding='utf-8')\n",
    "\n",
    "# 把train的資料串在一起\n",
    "train_data = laptop_train.append(restaurant_train)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "#把test的資料串在一起\n",
    "test_data = laptop_test.append(restaurant_test)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "#把train、test資料串在一起\n",
    "data = train_data.append(test_data)\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# data切割text\n",
    "data = split_text(data)\n",
    "\n",
    "print('訓練資料集:', len(train_data))\n",
    "print('測試資料集:', len(test_data))\n",
    "print('所有資料集:', len(data))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is of high quality, has a killer GUI, is extremely stable, is highly expandable, is bundled with lots of very good applications, is easy to use, and is absolutely gorgeous.\n",
      "\n",
      "it is of high quality\n",
      "\n",
      "quality has a killer gui is extremely stable is highly expandable is bundled with lots of very good applications is easy to use and is absolutely gorgeous\n",
      "\n",
      "it is of high quality quality has a killer gui is extremely stable is highly expandable is bundled with lots of very good applications is easy to use and is absolutely gorgeous\n"
     ]
    }
   ],
   "source": [
    "# print一個出來看看\n",
    "n = 5\n",
    "print(data.loc[n, 'text'])\n",
    "print()\n",
    "print(data.loc[n, 'left_text'])\n",
    "print()\n",
    "print(data.loc[n, 'right_text'])\n",
    "print()\n",
    "print(data.loc[n, 'left_right_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspect</th>\n",
       "      <th>polarity</th>\n",
       "      <th>left_text</th>\n",
       "      <th>right_text</th>\n",
       "      <th>left_right_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>cord</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i charge it at night and skip taking the cord</td>\n",
       "      <td>cord with me because of the good battery life</td>\n",
       "      <td>i charge it at night and skip taking the cord ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>battery life</td>\n",
       "      <td>positive</td>\n",
       "      <td>i charge it at night and skip taking the cord ...</td>\n",
       "      <td>battery life</td>\n",
       "      <td>i charge it at night and skip taking the cord ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>service center</td>\n",
       "      <td>negative</td>\n",
       "      <td>the tech guy then said the service center</td>\n",
       "      <td>service center does not do 1 to 1 exchange and...</td>\n",
       "      <td>the tech guy then said the service center serv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>\"sales\" team</td>\n",
       "      <td>negative</td>\n",
       "      <td>the tech guy then said the service center does...</td>\n",
       "      <td>sales team which is the retail shop which i bo...</td>\n",
       "      <td>the tech guy then said the service center does...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>tech guy</td>\n",
       "      <td>neutral</td>\n",
       "      <td>the tech guy</td>\n",
       "      <td>tech guy then said the service center does not...</td>\n",
       "      <td>the tech guy tech guy then said the service ce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          aspect  \\\n",
       "0  I charge it at night and skip taking the cord ...            cord   \n",
       "1  I charge it at night and skip taking the cord ...    battery life   \n",
       "2  The tech guy then said the service center does...  service center   \n",
       "3  The tech guy then said the service center does...    \"sales\" team   \n",
       "4  The tech guy then said the service center does...        tech guy   \n",
       "\n",
       "   polarity                                          left_text  \\\n",
       "0   neutral      i charge it at night and skip taking the cord   \n",
       "1  positive  i charge it at night and skip taking the cord ...   \n",
       "2  negative          the tech guy then said the service center   \n",
       "3  negative  the tech guy then said the service center does...   \n",
       "4   neutral                                       the tech guy   \n",
       "\n",
       "                                          right_text  \\\n",
       "0      cord with me because of the good battery life   \n",
       "1                                       battery life   \n",
       "2  service center does not do 1 to 1 exchange and...   \n",
       "3  sales team which is the retail shop which i bo...   \n",
       "4  tech guy then said the service center does not...   \n",
       "\n",
       "                                     left_right_text  label  \n",
       "0  i charge it at night and skip taking the cord ...      1  \n",
       "1  i charge it at night and skip taking the cord ...      2  \n",
       "2  the tech guy then said the service center serv...      0  \n",
       "3  the tech guy then said the service center does...      0  \n",
       "4  the tech guy tech guy then said the service ce...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把文字Label變成數字label\n",
    "data.loc[data['polarity'] == 'positive', 'label'] = 2\n",
    "data.loc[data['polarity'] == 'neutral', 'label'] = 1\n",
    "data.loc[data['polarity'] == 'negative', 'label'] = 0\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_text與right_text最多的字數: 72\n"
     ]
    }
   ],
   "source": [
    "#找出left_text跟right_text裡面最多是多少字\n",
    "max_count = 0\n",
    "for i in range(len(data)):\n",
    "    left_text_word_count = len(data.loc[i,'left_text'].split())\n",
    "    right_text_word_count = len(data.loc[i,'right_text'].split())\n",
    "    big_count = max(left_text_word_count, right_text_word_count)\n",
    "    if big_count>max_count:\n",
    "        max_count = big_count\n",
    "print('left_text與right_text最多的字數:', max_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 對文字作encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 7000 # 最大的字數\n",
    "max_seq_length = 80 # 句子最長長度\n",
    "embedding_dim = 300 # 每個字維度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6557 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# 把字變成token\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(data['left_right_text'].to_numpy())\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# word_index就是根據left_right_text內容彙整出來的切字跟代表那個字的token number (每個字的dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the : 1\n",
      "and : 2\n",
      "a : 3\n",
      "to : 4\n",
      "is : 5\n",
      "i : 6\n",
      "of : 7\n",
      "for : 8\n",
      "food : 9\n",
      "it : 10\n"
     ]
    }
   ],
   "source": [
    "# 檢查word_index(dictionary)裡面的東西，前面是字，後面是token\n",
    "for x in list(word_index)[0:10]:\n",
    "    print (x, ':', word_index[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one night i turned the freaking thing off after using it the next day i turn it on no gui\n",
      "gui screen all dark power light steady hard drive light steady and not flashing as it usually does\n",
      "[51, 267, 6, 1211, 1, 1648, 161, 236, 92, 292, 10, 1, 358, 315, 6, 1007, 10, 20, 59, 1530]\n",
      "[1530, 55, 33, 719, 148, 410, 1781, 100, 101, 410, 1781, 2, 22, 2934, 30, 10, 448, 213]\n",
      "<class 'list'>\n",
      "right text 倒過來\n",
      "[51, 267, 6, 1211, 1, 1648, 161, 236, 92, 292, 10, 1, 358, 315, 6, 1007, 10, 20, 59, 1530]\n",
      "[213, 448, 10, 30, 2934, 22, 2, 1781, 410, 101, 100, 1781, 410, 148, 719, 33, 55, 1530]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 檢查其中一項字串的token\n",
    "n = 15 # index number\n",
    "left_text = data['left_text'].to_numpy() # 轉成向量\n",
    "right_text = data['right_text'].to_numpy()\n",
    "left_text_seq = tokenizer.texts_to_sequences(left_text)\n",
    "right_text_seq = tokenizer.texts_to_sequences(right_text)\n",
    "print(data.loc[n, 'left_text'])\n",
    "print(data.loc[n, 'right_text'])\n",
    "print(left_text_seq[n])\n",
    "print(right_text_seq[n])\n",
    "print(type(right_text_seq))\n",
    "# 把右邊的字串token倒過來，因為要從後面讀到前面\n",
    "print('right text 倒過來')\n",
    "for i in range(len(right_text_seq)):\n",
    "    right_text_seq[i] = right_text_seq[i][::-1]\n",
    "print(left_text_seq[n])\n",
    "print(right_text_seq[n])\n",
    "print(type(right_text_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  51  267    6 1211    1 1648  161  236   92  292   10    1  358  315\n",
      "    6 1007   10   20   59 1530    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "[ 213  448   10   30 2934   22    2 1781  410  101  100 1781  410  148\n",
      "  719   33   55 1530    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# token sequence 後面補0的方法\n",
    "def text_seq_padding(text_seq):\n",
    "    if len(text_seq) < max_seq_length:\n",
    "        n = max_seq_length - len(text_seq)\n",
    "        text_seq = np.pad(text_seq, (0, n), mode ='constant', constant_values=(0)) # array右邊append n 個 0\n",
    "    return text_seq\n",
    "# 把每個left_text_seq，right_text_seq padding到同樣的長度 (後面補0)\n",
    "left_text_seq = [text_seq_padding(i) for i in left_text_seq] # 必須要 [ ] 輸出是list\n",
    "left_text_seq = np.array(left_text_seq)\n",
    "\n",
    "right_text_seq = [text_seq_padding(i) for i in right_text_seq]\n",
    "right_text_seq = np.array(right_text_seq)\n",
    "\n",
    "n = 15 # index number\n",
    "print(left_text_seq[n])\n",
    "print(right_text_seq[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用預先處理的詞向量 (crawl 300 dim)\n",
    "#### https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 載入詞向量\n",
    "# embeddings_index = {}\n",
    "# file = open('dataset/crawl-300d-2M.vec', 'r', encoding='utf-8')\n",
    "# for line in tqdm(file):\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# file.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNK = embeddings_index['UNK'] # unknown token\n",
    "# print(UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 根據得到的字典生成word_index裡每個字的詞向量\n",
    "# real_word = 0\n",
    "# embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))       #預設一個全部都是0的matrix，總共有每一個unique token的數量\n",
    "# for word, i in word_index.items():                                    #dict的index從1開始，所以np.zeros()數量要 +1\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector         #將找到的embedding vector丟到他位置的matrix, 如果找不到一樣維持0\n",
    "#         real_word = real_word + 1 # 看真正有找到的詞有幾個\n",
    "#     else:\n",
    "#         embedding_matrix[i] = UNK\n",
    "# print(embedding_matrix.shape)\n",
    "# print(embedding_matrix)\n",
    "# print('總共不重複的字數:', len(word_index))\n",
    "# print('在字典裡找到的字數:', real_word)\n",
    "# # embedding_matrix就是把word_index裡面的每個字所代表word embedding對應變成一個matrix (每個字的word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 把embedding_matrix.npy存起來，下次載入可以直接用\n",
    "# np.save('embedding_matrix', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(6558, 300)\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.0231      0.017       0.0157     ...  0.0744     -0.1118\n",
      "   0.0963    ]\n",
      " [-0.1081      0.0191      0.0354     ...  0.1104      0.0475\n",
      "  -0.0599    ]\n",
      " ...\n",
      " [ 0.16580001 -0.0169     -0.4138     ...  0.0933     -0.1168\n",
      "  -0.1777    ]\n",
      " [-0.1179      0.0726     -0.005      ...  0.2079      0.0322\n",
      "  -0.26879999]\n",
      " [ 0.24439999  0.1206      0.1123     ... -0.147      -0.0186\n",
      "  -0.3204    ]]\n"
     ]
    }
   ],
   "source": [
    "# 把embedding_matrix load 近來\n",
    "embedding_matrix = np.load('dataset/embedding_matrix.npy')\n",
    "print(type(embedding_matrix))\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertModel, TFBertForSequenceClassification, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer, to convert our text into tokens that correspond to BERT’s vocabulary.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7814, 2744, 19204]\n",
      "[7814, 2744]\n",
      "[101, 7814, 2744, 19204, 102]\n",
      "[101, 7814, 2744, 19204, 102, 7814, 2744, 102]\n"
     ]
    }
   ],
   "source": [
    "# 把aspect term token串在sep後面的練習\n",
    "text = 'aspect term token'\n",
    "aspect = 'aspect term'\n",
    "text_tok = tokenizer.tokenize(text) # 把文字變成token\n",
    "aspect_tok = tokenizer.tokenize(aspect)\n",
    "text_id = tokenizer.convert_tokens_to_ids(text_tok) # 把token變成Id\n",
    "aspect_id = tokenizer.convert_tokens_to_ids(aspect_tok)\n",
    "print(text_id)\n",
    "print(aspect_id)\n",
    "text_cls_sep = tokenizer.build_inputs_with_special_tokens(text_id) # 加入CLS、SEP token id\n",
    "print(text_cls_sep)\n",
    "text_sep_aspect = text_cls_sep + aspect_id\n",
    "text_sep_aspect.append(102)\n",
    "print(text_sep_aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 找出單句最多token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出最多text add aspect中最多是幾個token，不包含CLS跟SEP\n",
    "def find_max_token(pd):\n",
    "    max_token = 0\n",
    "    for i in range(len(pd)):\n",
    "        text = pd.loc[i, 'text']\n",
    "        aspect = pd.loc[i, 'aspect']\n",
    "        text_aspect = text + aspect\n",
    "        tokens_len = len(tokenizer.tokenize(text_aspect))\n",
    "        if tokens_len>max_token:\n",
    "            max_token = tokens_len\n",
    "    return max_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練資料集token最多是: 91\n",
      "測試資料集token最多是: 99\n"
     ]
    }
   ],
   "source": [
    "# 找出text add aspect中token最多的是幾個token，不包含CLS跟SEP\n",
    "train_max_token = find_max_token(train_data)\n",
    "test_max_token = find_max_token(test_data)\n",
    "print('訓練資料集token最多是:', train_max_token)\n",
    "print('測試資料集token最多是:', test_max_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正式把資料轉換成token(padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 把句子轉變成token(CLS+text+SEP+asepct)+(padding)的function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把維度固定在128維\n",
    "input_dim = 128\n",
    "def input_ids_all(pd):\n",
    "    pd['input_ids'] = 'N/A'\n",
    "    for i in range(len(pd)):\n",
    "        text = pd.loc[i, 'text']\n",
    "        aspect = pd.loc[i, 'aspect']\n",
    "        text_tokens = tokenizer.tokenize(text) # 把text轉成token\n",
    "        aspect_tokens = tokenizer.tokenize(aspect) # 把aspect轉成token\n",
    "        \n",
    "        text_input_ids = tokenizer.convert_tokens_to_ids(text_tokens) # 把text token轉成text token id\n",
    "        aspect_input_ids = tokenizer.convert_tokens_to_ids(aspect_tokens) # 把aspect token轉成aspect token id\n",
    "        \n",
    "        text_input_ids_cls = tokenizer.build_inputs_with_special_tokens(text_input_ids) # aspect token id加上CLS、SEP token id\n",
    "        input_ids = text_input_ids_cls + aspect_input_ids # 把aspect token id接在text token id 後面 (CLS+text+SEP+aspect)\n",
    "        input_ids.append(102)\n",
    "        input_ids = np.array(input_ids)\n",
    "        \n",
    "        if len(input_ids) < input_dim:\n",
    "            n = input_dim - len(input_ids)\n",
    "            input_ids = np.pad(input_ids, (0, n), mode ='constant', constant_values=(0)) # array右邊append n 個 0  補長度到512\n",
    "        \n",
    "        pd['input_ids'][i] = input_ids\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/bertenv2/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# 將text轉成token，後面加上aspect token存進dataframe\n",
    "data = input_ids_all(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把維度固定在128維\n",
    "input_dim = 128\n",
    "def input_ids_all_2(text, aspect):\n",
    "    text_tokens = tokenizer.tokenize(text) # 把text轉成token\n",
    "    aspect_tokens = tokenizer.tokenize(aspect) # 把aspect轉成token\n",
    "        \n",
    "    text_input_ids = tokenizer.convert_tokens_to_ids(text_tokens) # 把text token轉成text token id\n",
    "    aspect_input_ids = tokenizer.convert_tokens_to_ids(aspect_tokens) # 把aspect token轉成aspect token id\n",
    "        \n",
    "    text_input_ids_cls = tokenizer.build_inputs_with_special_tokens(text_input_ids) # aspect token id加上CLS、SEP token id\n",
    "    input_ids = text_input_ids_cls + aspect_input_ids # 把aspect token id接在text token id 後面 (CLS+text+SEP+aspect)\n",
    "    input_ids.append(102)\n",
    "    input_ids = np.array(input_ids)\n",
    "        \n",
    "    if len(input_ids) < input_dim:\n",
    "        n = input_dim - len(input_ids)\n",
    "        input_ids = np.pad(input_ids, (0, n), mode ='constant', constant_values=(0)) # array右邊append n 個 0  補長度到512\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['input_ids_2'] = data.apply(lambda column: input_ids_all_2(column['text'], column['aspect']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspect</th>\n",
       "      <th>polarity</th>\n",
       "      <th>left_text</th>\n",
       "      <th>right_text</th>\n",
       "      <th>left_right_text</th>\n",
       "      <th>label</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_ids_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>cord</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i charge it at night and skip taking the cord</td>\n",
       "      <td>cord with me because of the good battery life</td>\n",
       "      <td>i charge it at night and skip taking the cord ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1045, 3715, 2009, 2012, 2305, 1998, 1355...</td>\n",
       "      <td>[101, 1045, 3715, 2009, 2012, 2305, 1998, 1355...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>battery life</td>\n",
       "      <td>positive</td>\n",
       "      <td>i charge it at night and skip taking the cord ...</td>\n",
       "      <td>battery life</td>\n",
       "      <td>i charge it at night and skip taking the cord ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 1045, 3715, 2009, 2012, 2305, 1998, 1355...</td>\n",
       "      <td>[101, 1045, 3715, 2009, 2012, 2305, 1998, 1355...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        aspect  polarity  \\\n",
       "0  I charge it at night and skip taking the cord ...          cord   neutral   \n",
       "1  I charge it at night and skip taking the cord ...  battery life  positive   \n",
       "\n",
       "                                           left_text  \\\n",
       "0      i charge it at night and skip taking the cord   \n",
       "1  i charge it at night and skip taking the cord ...   \n",
       "\n",
       "                                      right_text  \\\n",
       "0  cord with me because of the good battery life   \n",
       "1                                   battery life   \n",
       "\n",
       "                                     left_right_text  label  \\\n",
       "0  i charge it at night and skip taking the cord ...      1   \n",
       "1  i charge it at night and skip taking the cord ...      2   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1045, 3715, 2009, 2012, 2305, 1998, 1355...   \n",
       "1  [101, 1045, 3715, 2009, 2012, 2305, 1998, 1355...   \n",
       "\n",
       "                                         input_ids_2  \n",
       "0  [101, 1045, 3715, 2009, 2012, 2305, 1998, 1355...  \n",
       "1  [101, 1045, 3715, 2009, 2012, 2305, 1998, 1355...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would have gotten some cole slaw and a knish if my stomach had more space.\n",
      "knish\n",
      "[  101  1045  2052  2031  5407  2070  5624 22889 10376  1998  1037 14161\n",
      "  4509  2065  2026  4308  2018  2062  2686  1012   102 14161  4509   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "[  101  1045  2052  2031  5407  2070  5624 22889 10376  1998  1037 14161\n",
      "  4509  2065  2026  4308  2018  2062  2686  1012   102 14161  4509   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "n = 4423\n",
    "print(data.loc[n, 'text'])\n",
    "print(data.loc[n, 'aspect'])\n",
    "print(data.loc[n, 'input_ids'])\n",
    "print(data.loc[n, 'input_ids_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7673, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  101,  1045,  3715, ...,     0,     0,     0],\n",
       "       [  101,  1045,  3715, ...,     0,     0,     0],\n",
       "       [  101,  1996,  6627, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101, 24519, 10439, ...,     0,     0,     0],\n",
       "       [  101, 24519, 10439, ...,     0,     0,     0],\n",
       "       [  101, 24519, 10439, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把data的input_ids提出存進list\n",
    "input_ids = list()\n",
    "for i in range(len(data)):\n",
    "    np_id = data.loc[i, 'input_ids']\n",
    "    input_ids.append(np_id)\n",
    "input_ids = np.array(input_ids)\n",
    "print(input_ids.shape)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切割train、test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X_train、Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5915 5915\n",
      "1758 1758\n"
     ]
    }
   ],
   "source": [
    "#把資料切割成train、test\n",
    "X_left_train = left_text_seq[:5915]\n",
    "X_right_train = right_text_seq[:5915]\n",
    "X_left_test = left_text_seq[5915:]\n",
    "X_right_test = right_text_seq[5915:]\n",
    "print(len(X_left_train), len(X_right_train))\n",
    "print(len(X_left_test), len(X_right_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5915\n",
      "1758\n"
     ]
    }
   ],
   "source": [
    "train_input_ids = input_ids[:5915]\n",
    "test_input_ids = input_ids[5915:]\n",
    "print(len(train_input_ids))\n",
    "print(len(test_input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Y_train、Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (7673,)\n",
      "5915\n",
      "1758\n"
     ]
    }
   ],
   "source": [
    "Y = data['label'].to_numpy() # label轉乘2維矩陣   # keras不吃1維label\n",
    "print('Shape of Y:', Y.shape)\n",
    "Y_train = Y[:5915]\n",
    "Y_test = Y[5915:]\n",
    "print(len(Y_train))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop_test           restaurant_test\n",
      "positive 2 2    positive 2 2\n",
      "negative 0 0    positive 2 2\n",
      "positive 2 2    positive 2 2\n",
      "negative 0 0    positive 2 2\n",
      "negative 0 0    positive 2 2\n",
      "negative 0 0    positive 2 2\n",
      "positive 2 2    positive 2 2\n",
      "negative 0 0    positive 2 2\n",
      "neutral 1 1    positive 2 2\n",
      "positive 2 2    positive 2 2\n",
      "positive 2 2    neutral 1 1\n",
      "positive 2 2    positive 2 2\n",
      "positive 2 2    positive 2 2\n",
      "positive 2 2    positive 2 2\n",
      "positive 2 2    negative 0 0\n",
      "positive 2 2    positive 2 2\n",
      "negative 0 0    neutral 1 1\n",
      "negative 0 0    neutral 1 1\n",
      "positive 2 2    positive 2 2\n",
      "positive 2 2    positive 2 2\n"
     ]
    }
   ],
   "source": [
    "# 檢查polarity跟label有沒有不一樣\n",
    "print('laptop_test', '         ','restaurant_test')\n",
    "for i in range(20):\n",
    "    print(laptop_test.loc[i, 'polarity'], data.loc[5915+i, 'label'], Y_test[i], '  ', restaurant_test.loc[i, 'polarity'], data.loc[6553+i, 'label'], Y_test[638+i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Flatten, InputLayer, Bidirectional, concatenate, add, average, Reshape\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把三邊input merge起來 (left text ht、right text ht、bert ht)，有加上dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 128, 768)     0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 98304)        0           dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 80, 300)      1967400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 80, 300)      1967400     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          50332160    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 128)          219648      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 128)          219648      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 768)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          196864      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            195         dropout_39[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 164,402,003\n",
      "Trainable params: 164,402,003\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# first input model 1\n",
    "input_layer_1 = Input(shape = (max_seq_length,), dtype='int64')\n",
    "embedding_1 = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], mask_zero=True, trainable=True)(input_layer_1)\n",
    "lstm_hidden_1 = LSTM(128, return_sequences=False, dropout=0.3)(embedding_1)\n",
    "\n",
    "# second input model 2\n",
    "input_layer_2 = Input(shape = (max_seq_length,), dtype='int64')\n",
    "embedding_2 = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], mask_zero=True, trainable=True)(input_layer_2)\n",
    "lstm_hidden_2 = LSTM(128, return_sequences=False, dropout=0.3)(embedding_2)\n",
    "\n",
    "# third input model 3\n",
    "input_layer_3= Input(shape = (128,), dtype='int64')\n",
    "bert = TFBertModel.from_pretrained('bert-base-uncased')(input_layer_3)\n",
    "bert = bert[0]\n",
    "dropout = Dropout(0.1)(bert)\n",
    "flat = Flatten()(dropout)\n",
    "bert_hidden = Dense(512)(flat)\n",
    "\n",
    "# merge input model\n",
    "# merge = concatenate([lstm_hidden_1, lstm_hidden_2, bert_hidden])\n",
    "merge = concatenate([bert_hidden, lstm_hidden_1, lstm_hidden_2])\n",
    "# merge = concatenate([lstm_hidden_1, lstm_hidden_2])\n",
    "hidden_1 = Dense(256, activation='relu')(merge)\n",
    "dropout_1 = Dropout(0.2)(hidden_1)\n",
    "hidden_2 = Dense(64, activation='relu')(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(hidden_2)\n",
    "output = Dense(3, activation='softmax')(dropout_2)\n",
    "model = Model(inputs=[input_layer_1, input_layer_2, input_layer_3], outputs=output)\n",
    "# model = Model(inputs=[input_layer_1, input_layer_2], outputs=output)\n",
    "print(model.summary())\n",
    "adam = Adam(lr=1e-3)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=8, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5915 samples, validate on 1758 samples\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "5915/5915 [==============================] - 154s 26ms/sample - loss: 1.0190 - accuracy: 0.5325 - val_loss: 0.9434 - val_accuracy: 0.6081\n",
      "Epoch 2/8\n",
      "5056/5915 [========================>.....] - ETA: 18s - loss: 1.0154 - accuracy: 0.5360"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit([X_left_train, X_right_train, train_input_ids],Y_train, batch_size=4,epochs=8,\n",
    "                      validation_data=([X_left_test, X_right_test, test_input_ids], Y_test))\n",
    "# model_fit = model.fit([X_left_train, X_right_train],Y_train, batch_size=64,epochs=30,\n",
    "#                       validation_data=([X_left_test, X_right_test], Y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
